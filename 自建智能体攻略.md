



 

# 本地双3090部署大模型

机器环境：win11 WSL安装Ubuntu 24.04.1 LTS
DeepSeek-R1-0528-Qwen3-8B 这个模型是从 DeepSeek-R1-0528 的思维链蒸馏出来用于后训练 Qwen3 8B Base 而得。通过蒸馏技术，在 AIME 2024 上达到 86.0，超越 Qwen3-8B (+10%)，媲美更大模型！
DeepSeek-R1-0528-Qwen3-8B在 2024 年美国数学邀请赛（AIME）上的开源模型中取得了最先进（SOTA）的性能，比 Qwen3 8B 提高了 10.0%，性能与 Qwen3-235B-thinking 相当。

创建虚拟环境

python -m venv myenv

source myenv/bin/activate

## 下载模型

模型文件：https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B/files
在下载前，先通过如下命令安装 ModelScope
```
pip install modelscope
```
命令行下载完整模型库
```
modelscope download --model deepseek-ai/DeepSeek-R1-0528-Qwen3-8B --local_dir .
```
模型大小约 16GB

## 部署
用 vllm 拉起大模型
```
pip install --upgrade vllm
CUDA_VIDIBLE_DEVICES=0,1 vllm serve . --served-model-name R1-0528-Qwen3-8B --port 8000  --tensor-parallel-size 2
```

## 测试
```
import requests

# API 地址
url = "http://192.168.1.9:8000/v1/chat/completions"

# 请求参数
headers = {
    "Content-Type": "application/json",
}

data = {
    "messages": [{"role": "user", "content": "描述一下北京的秋天"}]
}

# 发送请求
response = requests.post(url, headers=headers, json=data)

# 输出结果
if response.status_code == 200:
    print(response.json())
else:
    print(f"Error: {response.status_code}")
```

注意WSL环境下 输入hostname -I，这个命令会直接显示你的 WSL 系统的 IP 地址（如果有多个 IP 地址，它会列出所有）。

